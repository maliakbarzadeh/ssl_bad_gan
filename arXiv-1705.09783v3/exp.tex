
\section{Experiments}
\label{sec:exp}

% \subsection{Experiment settings}
%In this section, we conduct empirical experiments to evaluate the effectiveness of proposed approach.
% Following recent work in semi-supervised learning, 
We mainly consider three widely used benchmark datasets, namely MNIST, SVHN, and CIFAR-10.
As in previous work, we randomly sample 100, 1,000, and 4,000 labeled samples for MNIST, SVHN, and CIFAR-10 respectively during training, and use the standard data split for testing.
% For testing, we stick to the standard data split.
We use the 10-quantile log probability to define the threshold $\epsilon$ in Eq. \eqref{eq:g_full}. We add instance noise to the input of the discriminator \citep{arjovsky2017towards,sonderby2016amortised}, and use spatial dropout \citep{tompson2015efficient} to obtain faster convergence.
% In addition to the proposed approach in the previous section, we employ some additional techniques to stabilize and speedup the training. 
% Specifically, to alleviate the singular problem of the GAN discriminator, we add instance noise to the input of the discriminator as proposed by \citep{arjovsky2017towards} and \citep{sonderby2016amortised}.
% Meanwhile, we find spatial dropout~\citep{tompson2015efficient} often leads to faster and more stable than the standard dropout used in the original FM~\citep{salimans2016improved}.
Except for these two modifications, we use the same neural network architecture as in \citep{salimans2016improved}.
% , making sure the number of parameters remains the same.
% On the other hand, for simplicity, we do not keep an exponential moving average of the model parameters used at test time, a technique used by~\citep{salimans2016improved} to reduce variance.
For fair comparison, we also report the performance of our FM implementation with the aforementioned differences.
% For more implementation details and hyper-parameters, please refer to our code base\footnote{\url{https://github.com/anonymous}}.

\subsection{Main Results}
\begin{table}[t!]
\small
\centering
\begin{tabular}{ l c c c }
\toprule
Methods                                                           & MNIST (\# errors)& SVHN (\% errors) & CIFAR-10 (\% errors) \\ \midrule
CatGAN~\citep{springenberg2015unsupervised} & 191 $\pm$ 10      & -                          & 19.58 $\pm$ 0.46 \\
SDGM~\citep{maaloe2016auxiliary}                      & 132 $\pm$ 7       & 16.61 $\pm$ 0.24 & - \\
Ladder network~\citep{rasmus2015semi}            & 106 $\pm$ 37     & -                           & 20.40 $\pm$ 0.47 \\
ADGM~\citep{maaloe2016auxiliary}                      & 96 $\pm$ 2        & 22.86                   & - \\
FM~\citep{salimans2016improved}$\ ^*$              & 93 $\pm$ 6.5     & 8.11 $\pm$ 1.3      & 18.63 $\pm$ 2.32 \\
ALI~\citep{donahue2016adversarial}                    & -                        & 7.42 $\pm$ 0.65   & 17.99 $\pm$ 1.62 \\
VAT small~\citep{miyato2017virtual}$\ ^*$                           & 136                    & 6.83                     & 14.87 \\
Our best model$\ ^*$                                                 & \textbf{79.5 $\pm$ 9.8}   & \textbf{4.25 $\pm$ 0.03}     & \textbf{14.41 $\pm$ 0.30} \\ \midrule
Triple GAN~\citep{li2017triple}$\ ^{*\ddagger}$                    & 91$\pm$ 58       & 5.77 $\pm$ 0.17    & 16.99 $\pm$ 0.36 \\
$\Pi$ model~\citep{laine2016temporal}$\ ^{\dagger\ddagger}$ & - & 5.43 $\pm$ 0.25 & 16.55 $\pm$ 0.29 \\
VAT+EntMin+Large~\citep{miyato2017virtual}$ ^\dagger$ & - & 4.28 & 13.15 \\
\bottomrule
\end{tabular}
\caption{\small Comparison with state-of-the-art methods on three benchmark datasets. Only methods without data augmentation are included. $*$ indicates using the same (small) discriminator architecture, $\dagger$ indicates using a larger discriminator architecture, and $\ddagger$ means self-ensembling.}
\label{tab:sota}
\vspace{-1em}
\end{table}
\begin{figure}[tb]
	\centering
	\begin{subfigure}[t]{0.24\textwidth}
	\includegraphics[width=\textwidth]{FIG/svhn_fm.png}
	\caption{\small FM on SVHN}
	\label{fig:fm-svhn}
	\end{subfigure}
	\begin{subfigure}[t]{0.24\textwidth}
	\includegraphics[width=\textwidth]{FIG/svhn_fm+pt+ent.png}
	\caption{\small Ours on SVHN}
	\end{subfigure}
	\begin{subfigure}[t]{0.24\textwidth}
	\includegraphics[width=\textwidth]{FIG/cifar_fm.png}
	\caption{\small FM on CIFAR}
	\label{fig:fm-cifar}
	\end{subfigure}
	\begin{subfigure}[t]{0.24\textwidth}
	\includegraphics[width=\textwidth]{FIG/cifar_fm+vi.png}
	\caption{\small Ours on CIFAR}
	\end{subfigure}
	\caption{\small Comparing images generated by FM and our model. FM generates collapsed samples, while our model generates diverse ``bad'' samples.}
	\label{fig:gen}
\vspace{-1em}
\end{figure}

% \begin{table}[t!]
% \small
% \centering
% \begin{tabular}{ l l | l l | l l}
% \toprule
% Setting & Error & Setting & Error & Setting & Max log-p \\ \midrule
% MNIST FM & 85 $\pm$ 11.7 & SVHN FM & 6.83 & MNIST FM & -297 \\
% MNIST FM+KL & 79.5 $\pm$ 9.8 & SVHN FM+VI & 5.29 & MNIST FM+KL & -659 \\
% CIFAR FM & 16.14 & SVHN FM+PT & 4.63 & SVHN FM & -5809 \\
% CIFAR FM+KL & 14.41 & SVHN FM+PT+Ent & 4.25 & SVHN FM+KL & -5919 \\
% CIFAR FM+KL+Ent & 15.82 & SVHN FM+KL(PT)+Ent & 4.19 & SVHN 10-quant & -5622 \\
% \bottomrule
% \end{tabular}
% \caption{\small Ablation study. \textit{FM} is feature matching. \textit{KL} is the KL divergence term in Eq. (\ref{eq:g_full}). \textit{VI} and \textit{PT} are two entropy maximization methods described in Section \ref{sec:entropy}. \textit{Ent} means the conditional entropy term in Eq. (\ref{eq:d_full}). \textit{Max log-p} is the maximum log probability of generated samples in a batch, evaluated by a PixelCNN++ model. \textit{10-quant} shows the 10-quantile of true image log probability. \textit{Error} means the number of misclassified examples on MNIST, and error rate (\%) on others.}
% \label{tab:ablation}
% \end{table}

We compare the the results of our best model with state-of-the-art methods on the benchmarks in Table \ref{tab:sota}. Our proposed methods consistently improve the performance upon feature matching. We achieve new state-of-the-art results on all the datasets when only small discriminator architecture is considered. Our results are also state-of-the-art on MNIST and SVHN among all single-model results, even when compared with methods using self-ensembling and large discriminator architectures.
% As we can see, our proposed methods consistently improve the performance upon FM as well as other baselines on all three datasets.
Finally, note that because our method is actually orthogonal to VAT~\citep{miyato2017virtual},
%which takes advantage of virtual adversarial samples to enhance the local smoothness of the classifier.
combining VAT with our presented approach should yield further performance improvement in practice.

\subsection{Ablation Study}
\begin{table}[t!]
	\small
	\centering
	\begin{tabular}{ l l | l l }
		\toprule
		Setting & Error & Setting & Error \\ \midrule
		MNIST FM & 85.0 $\pm$ 11.7 & CIFAR FM & 16.14 \\
		MNIST FM+VI & 86.5 $\pm$ 10.6 & CIFAR FM+VI & 14.41 \\
		MNIST FM+LD & 79.5 $\pm$ 9.8 & CIFAR FM+VI+Ent & 15.82 \\
		MNIST FM+LD+Ent & 89.2 $\pm$ 10.5 &  \\
		% \bottomrule \\
		\midrule \midrule
		Setting & Error & Setting & Max log-p \\ \midrule
		SVHN FM & 6.83 & MNIST FM & -297 \\
		SVHN FM+VI & 5.29 & MNIST FM+LD & -659 \\
		SVHN FM+PT & 4.63 & SVHN FM+PT+Ent & -5809 \\
		SVHN FM+PT+Ent & 4.25 & SVHN FM+PT+LD+Ent & -5919 \\
		SVHN FM+PT+LD+Ent & 4.19 & SVHN 10-quant & -5622 \\
		% \bottomrule
		\midrule\midrule
	\end{tabular}
	\begin{tabular}{l l l l l}
		% \toprule
		% \\
		Setting $\epsilon$ as $q$-th centile & $q=2$ & $q=10$ & $q=20$ & $q=100$ \\ \midrule
		Error on MNIST & 77.7 $\pm$ 6.1 & 79.5 $\pm$ 9.8 & 80.1 $\pm$ 9.6 & 85.0 $\pm$ 11.7 \\
		\bottomrule
	\end{tabular}
	\caption{\small Ablation study. \textit{FM} is feature matching. \textit{LD} is the low-density enforcement term in Eq. (\ref{eq:ce}). \textit{VI} and \textit{PT} are two entropy maximization methods described in Section \ref{sec:gen-ent}. \textit{Ent} means the conditional entropy term in Eq. (\ref{eq:d_full}). \textit{Max log-p} is the maximum log probability of generated samples, evaluated by a PixelCNN++ model. \textit{10-quant} shows the 10-quantile of true image log probability. \textit{Error} means the number of misclassified examples on MNIST, and error rate (\%) on others.}
	\label{tab:ablation}
	\vspace{-2em}
\end{table}

We report the results of ablation study in Table \ref{tab:ablation}. In the following, we analyze the effects of several components in our model, subject to the intrinsic features of different datasets.

First, the generator entropy terms (VI and PT) (Section \ref{sec:gen-ent}) improve the performance on SVHN and CIFAR by up to 2.2 points in terms of error rate. Moreover, as shown in Fig \ref{fig:gen}, our model significantly reduces the collapsing effects present in the samples generated by FM, which also indicates that maximizing the generator entropy is beneficial. On MNIST, probably due to its simplicity, no collapsing phenomenon was observed with vanilla FM training~\cite{salimans2016improved} or in our setting. Under such circumstances, maximizing the generator entropy seems to be unnecessary, and the estimation bias introduced by approximation techniques can even hurt the performance. 

Second, the low-density (LD) term is useful when FM indeed generates samples in high-density areas. 
MNIST is a typical example in this case. When trained with FM, most of the generated hand written digits are highly realistic and have high log probabilities according to the density model (Cf. max log-p in Table \ref{tab:ablation}). 
Hence, when applied to MNIST, LD improves the performance by a clear margin.
By contrast, few of the generated SVHN images are realistic (Cf. Fig. \ref{fig:fm-svhn}). Quantitatively, SVHN samples are assigned very low log probabilities (Cf. Table \ref{tab:ablation}). 
As expected, LD has a negligible effect on the performance for SVHN. 
Moreover, the ``max log-p'' column in Table \ref{tab:ablation} shows that while LD can reduce the maximum log probability of the generated MNIST samples by a large margin, it does not yield noticeable difference on SVHN. This further justifies our analysis.
Based on the above conclusion, we conjecture LD would not help on CIFAR where sample quality is even lower. Thus, we did not train a density model on CIFAR due to the limit of computational resources.

%Second, the low-density (LD) term is useful on \textit{easy} datasets but not on \textit{hard} datasets. MNIST is an ``easy'' dataset since 1) the generated images look realistic \cite{salimans2016improved}, and 2) the samples generated by FM have high log probabilities (Cf. max log-p in Table \ref{tab:ablation}). By contrast, SVHN is relatively ``harder'' since 1) the generated images are much less realistic (Cf. Fig. \ref{fig:fm-svhn}), and 2) the samples generated by FM have low log probabilities (Cf. Table \ref{tab:ablation}). As a result, LD improves the performance on MNIST but not on SVHN. Moreover, the ``max log-p'' column in Table \ref{tab:ablation} shows LD reduces the maximum log probability of the generated samples by a large margin on MNIST but does not yield noticeable difference on SVHN, which further justifies our analysis. Based on the above conclusion, we conjecture LD would not help on CIFAR since it is even ``harder'' than SVHN, and thus we did not train a density model on CIFAR due to the limit of computational resources.

Third, adding the conditional entropy term has mixed effects on different datasets. 
While the conditional entropy (Ent) is an important factor of achieving the best performance on SVHN, it hurts the performance on MNIST and CIFAR. One possible explanation relates to the classic exploitation-exploration tradeoff, where minimizing Ent favors exploitation and minimizing the classification loss favors exploration. During the initial phase of training, the discriminator is relatively {\em uncertain} and thus the gradient of the Ent term might dominate. As a result, the discriminator learns to be more confident even on incorrect predictions, and thus gets trapped in local minima.

% One possible explanation relates to the classic exploitation-exploration trade off, where enforcing low conditional entropy encourages exploitation and limits exploration. Hence, the discriminator can get stuck in a local area. Another possibility is that when the top prediction is incorrect, the term can mistakenly make the discriminator more confident of the wrong answer. Experimentally, we observe an unstable training on CIFAR and conjecture there can also be optimization issues.

%Third, adding the conditional entropy term improves the performance on SVHN but decreases the performance on MNIST and CIFAR-10. With the conditional entropy term, we observe unstable training on CIFAR-10 and conjecture this is an optimization issue.

Lastly, we vary the values of the hyper-parameter $\epsilon$ in Eq. (\ref{eq:g_full}). As shown at the bottom of Table \ref{tab:ablation}, reducing $\epsilon$ clearly leads to better performance, which further justifies our analysis in Sections \ref{sec:case_study} and \ref{sec:theory} that off-manifold samples are favorable.

% \end{itemize}

% Our proposed techniques consistently lead to improvements in many cases, which verifies our analysis in previous sections. However, adding the conditional entropy term on CIFAR-10 decreases the performance --- we observe unstable training and conjecture that this is an optimization issue. Also, adding the cross entropy term (second term in Eq. \eqref{eq:g_full}) on SVHN does not lead to large improvement. As shown in the right column of Table \ref{tab:ablation}, samples generated by FM on SVHN are already of low probability, so further adding the cross entropy term does not have significant effects. On the other hand, FM samples on MNIST have high log probability, so the cross entropy term leads to gains in performance.

\subsection{Generated Samples}

% \textbf{Generated samples.}
We compare the generated samples of FM and our approach in Fig. \ref{fig:gen}. The FM images in Fig. \ref{fig:fm-cifar} are extracted from previous work \citep{salimans2016improved}. While collapsing is widely observed in FM samples, our model generates diverse ``bad'' images, which is consistent with our analysis.

% \begin{figure}[!tb]
% 	\centering
% 	\minipage{0.5\textwidth}\centering
% 	\includegraphics[width=\linewidth]{FIG/verify_assump_2_3.png}
% 	\label{fig:verify_assumption}
% 	\caption{}
% 	\endminipage
% \end{figure}
