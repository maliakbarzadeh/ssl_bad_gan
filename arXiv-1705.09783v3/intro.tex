
\section{Introduction} \label{sec:intro}

Deep neural networks are usually trained on a large amount of labeled data, and it has been a challenge to apply deep models to datasets with limited labels. Semi-supervised learning (SSL) aims to leverage the large amount of unlabeled data to boost the model performance, particularly focusing on the setting where the amount of available labeled data is limited. Traditional graph-based methods \citep{belkin2006manifold,zhu2003semi} were extended to deep neural networks \citep{weston2012deep,yang2016revisiting,kipf2016semi}, which involves applying convolutional neural networks \citep{lecun1998gradient} and feature learning techniques to graphs so that the underlying manifold structure can be exploited. \citep{rasmus2015semi} employs a Ladder network to minimize the layerwise reconstruction loss in addition to the standard classification loss. Variational auto-encoders have also been used for semi-supervised learning \citep{kingma2014semi,maaloe2016auxiliary} by maximizing the variational lower bound of the unlabeled data log-likelihood.

Recently, generative adversarial networks (GANs) \citep{goodfellow2014generative} were demonstrated to be able to generate visually realistic images. GANs set up an adversarial game between a discriminator and a generator. The goal of the discriminator is to tell whether a sample is drawn from true data or generated by the generator, while the generator is optimized to generate samples that are not distinguishable by the discriminator. Feature matching (FM) GANs \citep{salimans2016improved} apply GANs to semi-supervised learning on $K$-class classification. The objective of the generator is to match the first-order feature statistics between the generator distribution and the true distribution. Instead of binary classification, the discriminator employs a $(K+1)$-class objective, where true samples are classified into the first $K$ classes and generated samples are classified into the $(K+1)$-th class. This $(K+1)$-class discriminator objective leads to strong empirical results, and was later widely used to evaluate the effectiveness of generative models \citep{dumoulin2016adversarially,ulyanov2017adversarial}.


% In this project, we consider the semi-supervised learning (SSL) task for image classification. 
% Recently neural network models have achieved great success on computer vision tasks, including image classification. 
% The success of these methods relies on large amount of labeled data. 
% For example, the popular ImageNet~\citep{russakovsky2015imagenet} dataset has over 14 million images. 
% However, such large-scale labeled datasets may not always be available. 
% It is time-consuming and expensive to create human annotated datasets. 
% This motivates us to study semi-supervised learning problems. 
% Though labeled data are expensive to get, there are many unlabeled data available on the Internet. 
% In this project we study how to leverage unlabeled data to boost the performance of image classification. 

% Recently generative adversarial network (GAN)~\citep{goodfellow2014generative} has been applied to semi-supervised image classification and achieved state-of-art result~\citep{salimans2016improved}. 
% Given a dataset of (possibly unlabeled) real images, the key idea of GAN is to jointly learn two neural networks, \emph{discriminator $D$} and \emph{generator $G$}, that model the distribution of these images.
% More concretely, the goal of the generator $G$ is to generate images that looks like real images, and the goal of the discriminator $D$ is to tell whether the image is real or generated.
% The output of $D$ is often the probability that the input image is from the true distribution.
% It is possible to do semi-supervised learning with any standard classifier by augmenting the dataset with images generated by $G$.
% Consider a classification task that maps data $\mathbf{x}$ into $K$ classes, we can label generated data with a new class $K + 1$.
% The loss function in this case becomes:
% \begin{align}
% L = &- \mathbb{E}_{\mathbf{x}, y \sim p_{\text{data}(\mathbf{x}, y)}} [ \log p_\text{model} (y \mid \mathbf{x})] \nonumber \\
% &- \mathbb{E}_{\mathbf{x} \sim G}[ \log p_\text{model}(y = K+1 \mid \mathbf{x})] \label{eq:ssl_gan_loss}
% \end{align}

% Additionally, a feature matching technique has been introduced to improve semi-supervised learning. 
% Feature matching differs from classical GAN models in the objective function. 
% While in classical GAN, the generator $G$ wants to maximize the output of $D$. 
% In feature matching, $G$ wants to generate data that match the statistics of real data. 
% Mathematically, let $f(\mathbf{x})$ be the activation function on intermediate layer of the discriminator, the new objective function for $G$ is 
% \begin{equation}
% \| 
% \mathbb{E}_{\mathbf{x} \sim p_\text{data}} f(\mathbf(x)) 
% - \mathbb{E}_{\mathbf{z} \sim p_\mathbf{z}} f(G(\mathbf{z}))
% \|_2^2
% \end{equation}

Though empirically feature matching improves semi-supervised classification performance, the following questions still remain open. First, it is not clear why the formulation of the discriminator can improve the performance when combined with a generator. Second, it seems that good semi-supervised learning and a good generator cannot be obtained at the same time. For example,~\citep{salimans2016improved} observed that mini-batch discrimination generates better images than feature matching, but feature matching obtains a much better semi-supervised learning performance. The same phenomenon was also observed in \citep{ulyanov2017adversarial}, where the model generated better images but failed to improve the performance on semi-supervised learning.
% It is not clear why these are happening because intuitively a good generator should help SSL.

In this work, we take a step towards addressing these questions. First, we show that given the current $(K+1)$-class discriminator formulation of GAN-based SSL, good semi-supervised learning requires a ``bad'' generator. Here by \textit{bad} we mean the generator distribution should not match the true data distribution. Then, we give the definition of a preferred generator, which is to generate complement samples in the feature space. Theoretically, under mild assumptions, we show that a properly optimized discriminator obtains correct decision boundaries in high-density areas in the feature space if the generator is a \textit{complement generator}.

Based on our theoretical insights, we analyze why feature matching works on 2-dimensional toy datasets. It turns out that our practical observations align well with our theory. However, we also find that the feature matching objective has several drawbacks. Therefore, we develop a novel formulation of the discriminator and generator objectives to address these drawbacks. In our approach, the generator minimizes the KL divergence between the generator distribution and a target distribution that assigns high densities for data points with low densities in the true distribution, which corresponds to the idea of a complement generator. Furthermore, to enforce our assumptions in the theoretical analysis, we add the conditional entropy term to the discriminator objective.

Empirically, our approach substantially improves over vanilla feature matching GANs, and obtains new state-of-the-art results on MNIST, SVHN, and CIFAR-10 when all methods are compared under the same discriminator architecture. Our results on MNIST and SVHN also represent state-of-the-art amongst all single-model results.

% We further leverage our theoretical insight to develop novel techniques to explicitly optimize our objective.
% Empirically, we show improvement over state-of-the-art results, and demonstrate our theoretical insight on synthetic datasets.
